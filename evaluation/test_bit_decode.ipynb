{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddy/miniconda3/envs/bitdecode/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import triton\n",
    "\n",
    "import numpy as np\n",
    "import bit_decode_cuda as bit_decode_cuda\n",
    "from bit_decode import kvcache_pack_int, fwd_kvcache_int\n",
    "from bit_decode import DynamicCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_ref(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        q: (batch_size, seqlen_q, nheads, head_dim)\n",
    "        k: (batch_size, seqlen_k, nheads_k, head_dim)\n",
    "        v: (batch_size, seqlen_k, nheads_k, head_dim)\n",
    "    Output:\n",
    "        output: (batch_size, seqlen_q, nheads, head_dim)\n",
    "        attention: (batch_size, nheads, seqlen_q, seqlen_k), softmax after dropout\n",
    "    \"\"\"\n",
    "    dtype_og = q.dtype\n",
    "\n",
    "    d = q.shape[-1]\n",
    "\n",
    "    scores = torch.einsum(\"bthd,bshd->bhts\", q / math.sqrt(d), k)\n",
    "    \n",
    "    attention = torch.softmax(scores, dim=-1).to(v.dtype)\n",
    "\n",
    "    output = torch.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "\n",
    "    return output.to(dtype=dtype_og), attention.to(dtype=dtype_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization parameters\n",
    "quant_mode = \"k-channel\"\n",
    "num_bits = 4\n",
    "pack_nums = 16 / num_bits\n",
    "group_size = 32\n",
    "residual_block_size = 128\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "layer_idx = 0\n",
    "batch_size = 1\n",
    "nheads = 32\n",
    "nheads_k = 32\n",
    "d = 128\n",
    "\n",
    "seqlen_q = 1\n",
    "seqlen_k = 1024\n",
    "sm_scale = 1.0 / math.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m     past_key_value\u001b[38;5;241m.\u001b[39mupdate_pack(k_pack_new, k_params_new, v_pack_new, v_params_new, layer_idx)\n\u001b[1;32m     95\u001b[0m     past_key_value\u001b[38;5;241m.\u001b[39mclear_residual(layer_idx)\n\u001b[0;32m---> 97\u001b[0m k_state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_new\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m v_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v_state, v_new], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m out_ref \u001b[38;5;241m=\u001b[39m attention_ref(q, k_state, v_state)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "####### Round 1 : Prefill #######\n",
    "torch.manual_seed(42)\n",
    "\n",
    "q = torch.rand(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype)\n",
    "k_state = torch.randn(batch_size, seqlen_k, nheads_k, d, device=device, dtype=dtype)\n",
    "v_state = torch.randn(batch_size, seqlen_k, nheads_k, d, device=device, dtype=dtype)\n",
    "\n",
    "residual_len = seqlen_k % residual_block_size\n",
    "residual     = residual_len > 0\n",
    "seqlen_k_pack = seqlen_k - residual_len\n",
    "\n",
    "cu_seqlens_k = torch.arange(0, (batch_size + 1) * seqlen_k_pack, seqlen_k_pack, \n",
    "                           dtype=torch.int32, device=device)\n",
    "\n",
    "# Initialize quantization tensors\n",
    "if quant_mode == \"k-channel\":\n",
    "    k_pack   = torch.zeros((batch_size, int(seqlen_k_pack // pack_nums), nheads_k, d),  dtype=torch.uint16, device=device)\n",
    "    k_params = torch.zeros((batch_size, int(seqlen_k_pack // group_size), nheads_k, d), dtype=torch.float32, device=device)\n",
    "else:\n",
    "    k_pack   = torch.zeros((batch_size, seqlen_k_pack, nheads_k, int(d // pack_nums)),  dtype=torch.uint16, device=device)\n",
    "    k_params = torch.zeros((batch_size, int(d // group_size), nheads_k, seqlen_k_pack), dtype=torch.float32, device=device)\n",
    "\n",
    "v_pack   = torch.zeros((batch_size, seqlen_k_pack, nheads_k, int(d // pack_nums)),  dtype=torch.uint16, device=device)\n",
    "v_params = torch.zeros((batch_size, int(d // group_size), nheads_k, seqlen_k_pack), dtype=torch.float32, device=device)\n",
    "\n",
    "# KV Cache Dynamic Cache\n",
    "past_key_value = DynamicCache()\n",
    "\n",
    "if residual:\n",
    "    k_state_residual = k_state[:, -residual_len:, :, :]\n",
    "    v_state_residual = v_state[:, -residual_len:, :, :]\n",
    "    k_state_past = k_state[:, :-residual_len, :, :]\n",
    "    v_state_past = v_state[:, :-residual_len, :, :]\n",
    "    past_key_value.update_residual(k_state_residual, v_state_residual, layer_idx)\n",
    "else:\n",
    "    k_state_past = k_state\n",
    "    v_state_past = v_state\n",
    "\n",
    "kvcache_pack_int(\n",
    "    k_state_past, k_pack, k_params,\n",
    "    v_state_past, v_pack, v_params,\n",
    "    None, # opt_block_table\n",
    "    cu_seqlens_k,              \n",
    "    seqlen_k_pack,\n",
    "    quant_mode,\n",
    "    group_size,\n",
    "    num_bits\n",
    ")\n",
    "past_key_value.update_pack(k_pack, k_params, v_pack, v_params, layer_idx)\n",
    "\n",
    "# self\n",
    "k_pack_new = torch.empty((batch_size, int(residual_block_size // pack_nums), nheads_k, k_pack.size(-1)),  dtype=torch.uint16, device=device)\n",
    "k_params_new = torch.empty((batch_size, int(residual_block_size // group_size), nheads_k, k_params.size(-1)), dtype=torch.float32, device=device)\n",
    "v_pack_new = torch.empty((batch_size, residual_block_size, nheads_k, v_pack.size(-1)), dtype=torch.uint16, device=device)\n",
    "v_params_new = torch.empty((batch_size, v_params.size(1), nheads_k, residual_block_size), dtype=torch.float32, device=device)\n",
    "\n",
    "####### Round 2-3 : Decode #######\n",
    "for round_idx in range(250):\n",
    "    k_new = torch.randn(batch_size, 1, nheads_k, d, device=device, dtype=dtype)\n",
    "    v_new = torch.randn(batch_size, 1, nheads_k, d, device=device, dtype=dtype)\n",
    "\n",
    "    # Get kv cache_pack\n",
    "    k_pack, k_params, v_pack, v_params = past_key_value.update_pack(None, None, None, None, layer_idx)\n",
    "\n",
    "    seqlen_pack = v_pack.shape[1]\n",
    "    seqlens_k = torch.full((batch_size,), seqlen_pack, dtype=torch.int32, device=device)\n",
    "\n",
    "    # Get kv cache_residual and append new kv\n",
    "    k_residual = torch.zeros((batch_size, residual_block_size, nheads_k, d), device=device, dtype=dtype)\n",
    "    v_residual = torch.zeros((batch_size, residual_block_size, nheads_k, d), device=device, dtype=dtype)\n",
    "    k_residual_cache, v_residual_cache = past_key_value.update_residual(k_new, v_new, layer_idx)\n",
    "\n",
    "    cur_residual_len = k_residual_cache.shape[1]\n",
    "\n",
    "    k_residual[:, :cur_residual_len, :, :] = k_residual_cache\n",
    "    v_residual[:, :cur_residual_len, :, :] = v_residual_cache\n",
    "\n",
    "    out_bitdecode, k_pack_new, k_params_new, v_pack_new, v_params_new = fwd_kvcache_int(\n",
    "        q,\n",
    "        k_pack, k_params, \n",
    "        v_pack, v_params,\n",
    "        k_residual, v_residual, seqlens_k, #seqlens_k\n",
    "        k_pack_new, k_params_new, v_pack_new, v_params_new,\n",
    "        None, # opt_block_table\n",
    "        sm_scale,\n",
    "        quant_mode, \n",
    "        group_size,\n",
    "        residual_block_size,\n",
    "        cur_residual_len, # new_lens\n",
    "        num_bits\n",
    "    )\n",
    "\n",
    "    if cur_residual_len == residual_block_size:\n",
    "        past_key_value.update_pack(k_pack_new, k_params_new, v_pack_new, v_params_new, layer_idx)\n",
    "        past_key_value.clear_residual(layer_idx)\n",
    "\n",
    "    k_state = torch.cat([k_state, k_new], dim=1)\n",
    "    v_state = torch.cat([v_state, v_new], dim=1)\n",
    "\n",
    "    out_ref = attention_ref(q, k_state, v_state)[0]\n",
    "    print(f\"Round {round_idx+2}: bitdecode vs pytorch: {(out_bitdecode - out_ref).abs().mean().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitdecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
